{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "272cf066-3542-4b57-aa7f-b86e6de58b0b",
   "metadata": {},
   "source": [
    "## Import architectures and necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97c196dc-6398-4376-9e20-e1a4442f964b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 05:11:23.622922: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-10 05:11:24.192197: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "from densenet import DenseNet\n",
    "from glob import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdc3a3a-a1b7-44cc-a452-3f216c258537",
   "metadata": {},
   "source": [
    "## Preprocessing images for train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb45ab3c-5af8-47b6-855f-282e99030d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "images = []\n",
    "\n",
    "imagesfiles = [i for i in glob('data/*.JPEG')]\n",
    "for i in imagesfiles:\n",
    "    image = cv2.imread(i)/255.0\n",
    "    image = cv2.resize(image, (224,224))\n",
    "    images.append(image)\n",
    "images = np.array(images)\n",
    "\n",
    "with open('data/imagenet_2012_validation_synset_labels.txt') as f:\n",
    "    labelstxt = f.read().split('\\n')[:-1]\n",
    "with open('data/labels.txt') as f:\n",
    "    labs = f.read()\n",
    "    \n",
    "for i in labelstxt:\n",
    "    bosh = labs.find(i)\n",
    "    labels.append(int(labs[bosh+10:bosh+10+labs[bosh+10:].find(' ')])-1)\n",
    "    \n",
    "images = np.array(images)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b14e564-4f8a-44a4-a73f-53a38ce3cda3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 224, 224, 3) (40000,)\n",
      "(10000, 224, 224, 3) (10000,)\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=100)\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7646ff-e3ea-4ccd-a92e-29e6fc6f59b0",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6ebca4b-8ee5-46ad-835a-ff52c5477c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(keras.models.Model):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.base_model = DenseNet()\n",
    "    def call(self, x):\n",
    "        x = self.base_model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b07373a-a5ae-4c7a-b339-49f27876112a",
   "metadata": {},
   "source": [
    "## Define loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c5cae67-b967-40f4-aeb4-07c04986286e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-09 19:04:41.769632: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-09 19:04:41.769864: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-09 19:04:41.794532: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "model = Model()\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-3), loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feafb192-fe9a-480a-b35e-f80d73f4b193",
   "metadata": {},
   "source": [
    "## Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c8743bc-013f-4421-93d4-ae8bcb10fbf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-09 19:04:46.695226: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 19267584000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 3576s 2s/step - loss: 6.9523 - accuracy: 5.3125e-04 - val_loss: 6.9442 - val_accuracy: 6.2500e-04\n",
      "Epoch 2/5\n",
      "2000/2000 [==============================] - 3532s 2s/step - loss: 6.9131 - accuracy: 0.0011 - val_loss: 6.9369 - val_accuracy: 5.0000e-04\n",
      "Epoch 3/5\n",
      " 582/2000 [=======>......................] - ETA: 40:02 - loss: 6.9079 - accuracy: 9.6649e-04"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/keras/engine/training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1677\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1678\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1679\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1682\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1683\u001b[0m ):\n\u001b[1;32m   1684\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1685\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1687\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    891\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 894\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    896\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    897\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:926\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    923\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    924\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    925\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 926\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    927\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    928\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    929\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    930\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    141\u001b[0m   (concrete_function,\n\u001b[1;32m    142\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1753\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1754\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1755\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1756\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1757\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1758\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1759\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1760\u001b[0m     args,\n\u001b[1;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1762\u001b[0m     executing_eagerly)\n\u001b[1;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    380\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 381\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    387\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    388\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    389\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    390\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    393\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    394\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size=16, epochs=5, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9ed4de-73bc-4e08-bf62-ff8fda3ef98a",
   "metadata": {},
   "source": [
    "## Finetune architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac422e96-205c-4a35-a2c5-75e68ab6eb51",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 05:11:26.729342: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-10 05:11:26.729645: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-10 05:11:26.774482: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-05-10 05:11:26.851453: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype string and shape [4]\n",
      "\t [[{{node Placeholder/_1}}]]\n",
      "2023-05-10 05:11:26.851716: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_4' with dtype int64 and shape [4]\n",
      "\t [[{{node Placeholder/_4}}]]\n",
      "Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: premature end of data segment\n",
      "2023-05-10 05:11:29.159807: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_4' with dtype int64 and shape [4]\n",
      "\t [[{{node Placeholder/_4}}]]\n",
      "2023-05-10 05:11:29.160086: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_4' with dtype int64 and shape [4]\n",
      "\t [[{{node Placeholder/_4}}]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((3680, 224, 224, 3),\n",
       " (3680,),\n",
       " (3680, 1),\n",
       " (3669, 224, 224, 3),\n",
       " (3669,),\n",
       " (3669, 1))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = tfds.load('oxford_iiit_pet', split='train')\n",
    "test = tfds.load('oxford_iiit_pet', split='test')\n",
    "\n",
    "x_train, y_train, y_train2, x_test, y_test, y_test2 = [], [], [], [], [], []\n",
    "for i in train:\n",
    "    a = i['image'].numpy()\n",
    "    x_train.append(cv2.resize(a, (224, 224))/255.0)\n",
    "    y_train.append(i['label'].numpy())\n",
    "    y_train2.append(i['species'].numpy())\n",
    "    \n",
    "for i in test:\n",
    "    a = i['image'].numpy()\n",
    "    x_test.append(cv2.resize(a, (224, 224))/255.0)\n",
    "    y_test.append(i['label'].numpy())\n",
    "    y_test2.append(i['species'].numpy())\n",
    "    \n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "y_train2 = np.array(y_train2)\n",
    "y_train2 = np.expand_dims(y_train2, axis=1)\n",
    "\n",
    "x_test = np.array(x_test)\n",
    "y_test = np.array(y_test)\n",
    "y_test2 = np.array(y_test2)\n",
    "y_test2 = np.expand_dims(y_test2, axis=1)\n",
    "\n",
    "x_train.shape, y_train.shape, y_train2.shape, x_test.shape, y_test.shape, y_test2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "faeefdc9-c733-47ca-b257-eb6e5d2036b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification for two classes\n",
    "class Model1(keras.models.Model):\n",
    "    def __init__(self):\n",
    "        super(Model1, self).__init__()\n",
    "        self.base_model = DenseNet(include_top=False)\n",
    "        self.flatten = keras.layers.Flatten()\n",
    "        self.fc1 = keras.layers.Dense(1000, activation='relu')\n",
    "        self.fc2 = keras.layers.Dense(256, activation='relu')\n",
    "        self.out = keras.layers.Dense(1, activation='sigmoid')\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.base_model(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "    \n",
    "# classification for 37 classes\n",
    "class Model2(keras.models.Model):\n",
    "    def __init__(self):\n",
    "        super(Model2, self).__init__()\n",
    "        self.base_model = DenseNet(include_top=False)\n",
    "        self.flatten = keras.layers.Flatten()\n",
    "        self.fc1 = keras.layers.Dense(1000, activation='relu')\n",
    "        self.fc2 = keras.layers.Dense(256, activation='relu')\n",
    "        self.out = keras.layers.Dense(37, activation='softmax')\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.base_model(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "    \n",
    "model1 = Model1()\n",
    "model2 = Model2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c24160e0-cc4d-465e-b4e9-14d6964bac8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss and optimizer\n",
    "loss_object1 = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "optimizer1 = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "train_loss1 = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy1 = tf.keras.metrics.BinaryAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss1 = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy1 = tf.keras.metrics.BinaryAccuracy(name='test_accuracy')\n",
    "\n",
    "# optimize functions\n",
    "@tf.function\n",
    "def train_step1(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model1(images, training=True)\n",
    "        loss = loss_object1(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model1.trainable_variables)\n",
    "    optimizer1.apply_gradients(zip(gradients, model1.trainable_variables))\n",
    "\n",
    "    train_loss1(loss)\n",
    "    train_accuracy1(labels, predictions)\n",
    "\n",
    "@tf.function\n",
    "def test_step1(images, labels):\n",
    "    predictions = model1(images, training=False)\n",
    "    t_loss = loss_object1(labels, predictions)\n",
    "    \n",
    "    test_loss1(t_loss)\n",
    "    test_accuracy1(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13d81c1c-19c8-4e0f-98c8-b6024ce3750f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss and optimizer\n",
    "loss_object2 = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer2 = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "train_loss2 = tf.keras.metrics.Mean(name='train_loss2')\n",
    "train_accuracy2 = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy2')\n",
    "\n",
    "test_loss2 = tf.keras.metrics.Mean(name='test_loss2')\n",
    "test_accuracy2 = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy2')\n",
    "\n",
    "# optimize functions\n",
    "@tf.function\n",
    "def train_step2(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model2(images, training=True)\n",
    "        loss = loss_object2(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model2.trainable_variables)\n",
    "    optimizer2.apply_gradients(zip(gradients, model2.trainable_variables))\n",
    "\n",
    "    train_loss2(loss)\n",
    "    train_accuracy2(labels, predictions)\n",
    "\n",
    "@tf.function\n",
    "def test_step2(images, labels):\n",
    "    predictions = model2(images, training=False)\n",
    "    t_loss = loss_object2(labels, predictions)\n",
    "    \n",
    "    test_loss2(t_loss)\n",
    "    test_accuracy2(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6258f97-46ca-4688-955c-3dadb50dd961",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, x_set, y_set, batch_size):\n",
    "        self.x, self.y = x_set, y_set\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        return batch_x, batch_y\n",
    "\n",
    "# for 2 classes\n",
    "train_gen1 = DataGenerator(x_train, y_train2, 32)\n",
    "test_gen1 = DataGenerator(x_test, y_test2, 32)\n",
    "\n",
    "# for 37 classes\n",
    "train_gen2 = DataGenerator(x_train, y_train, 32)\n",
    "test_gen2 = DataGenerator(x_test, y_test, 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995acecd-107d-41c7-b604-61021e1ec86a",
   "metadata": {},
   "source": [
    "### Train 2 classes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d5e04c3-5e96-4f5a-af2a-c9f9216d8132",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/airi/anaconda3/lib/python3.9/site-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.407275676727295, Accuracy: 60.380435943603516%, Test Loss: 0.6294675469398499, Test Accuracy: 67.75688171386719%\n",
      "Epoch 1, Loss: 0.6554157137870789, Accuracy: 66.41304779052734%, Test Loss: 0.6284439563751221, Test Accuracy: 67.75688171386719%\n",
      "Epoch 2, Loss: 0.6224454641342163, Accuracy: 67.98912811279297%, Test Loss: 0.6413689851760864, Test Accuracy: 66.36685943603516%\n",
      "Epoch 3, Loss: 0.6004112958908081, Accuracy: 68.72282409667969%, Test Loss: 0.6910366415977478, Test Accuracy: 56.5549201965332%\n",
      "Epoch 4, Loss: 0.5863730907440186, Accuracy: 70.10869598388672%, Test Loss: 0.576470673084259, Test Accuracy: 69.25592803955078%\n",
      "Epoch 5, Loss: 0.5707728266716003, Accuracy: 71.1684799194336%, Test Loss: 0.6412765979766846, Test Accuracy: 70.89125061035156%\n",
      "Epoch 6, Loss: 0.5552626848220825, Accuracy: 72.66304779052734%, Test Loss: 0.6390237808227539, Test Accuracy: 71.2728271484375%\n",
      "Epoch 7, Loss: 0.5302475690841675, Accuracy: 73.8315200805664%, Test Loss: 0.5826123356819153, Test Accuracy: 72.90814971923828%\n",
      "Epoch 8, Loss: 0.49016013741493225, Accuracy: 76.60326385498047%, Test Loss: 0.5983931422233582, Test Accuracy: 72.60833740234375%\n",
      "Epoch 9, Loss: 0.4732210040092468, Accuracy: 77.5543441772461%, Test Loss: 0.5803318023681641, Test Accuracy: 72.79912567138672%\n",
      "Epoch 10, Loss: 0.45335498452186584, Accuracy: 78.91304779052734%, Test Loss: 0.5422258377075195, Test Accuracy: 76.06977081298828%\n",
      "Epoch 11, Loss: 0.4243437945842743, Accuracy: 80.59782409667969%, Test Loss: 0.6760987043380737, Test Accuracy: 73.45326232910156%\n",
      "Epoch 12, Loss: 0.4167119264602661, Accuracy: 81.14130401611328%, Test Loss: 0.5828782320022583, Test Accuracy: 75.03407287597656%\n",
      "Epoch 13, Loss: 0.4000895023345947, Accuracy: 81.63043212890625%, Test Loss: 0.6186766028404236, Test Accuracy: 71.21831512451172%\n",
      "Epoch 14, Loss: 0.38514527678489685, Accuracy: 81.90217590332031%, Test Loss: 0.7911292910575867, Test Accuracy: 70.64595031738281%\n"
     ]
    }
   ],
   "source": [
    "epochs = 15\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Reset the metrics at the start of the next epoch\n",
    "    train_loss1.reset_states()\n",
    "    train_accuracy1.reset_states()\n",
    "    test_loss1.reset_states()\n",
    "    test_accuracy1.reset_states()\n",
    "\n",
    "    for train_images, train_labels in train_gen1:\n",
    "        train_step1(train_images, train_labels)\n",
    "\n",
    "    for test_images, test_labels in test_gen1:\n",
    "        test_step1(test_images, test_labels)\n",
    "    if epoch % 1 == 0:\n",
    "        print(\n",
    "          f'Epoch {epoch}, '\n",
    "          f'Loss: {train_loss1.result()}, '\n",
    "          f'Accuracy: {train_accuracy1.result() * 100}%, '\n",
    "          f'Test Loss: {test_loss1.result()}, '\n",
    "          f'Test Accuracy: {test_accuracy1.result() * 100}%'\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bf1f76-980b-47a9-ac8a-f9e120d369b2",
   "metadata": {},
   "source": [
    "### Train 37 classes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8cba3ade-8941-483e-a9da-96c282afc78a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/airi/anaconda3/lib/python3.9/site-packages/keras/backend.py:5612: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 6.428648471832275, Accuracy: 3.6141304969787598%, Test Loss: 3.6413559913635254, Test Accuracy: 2.752793788909912%\n",
      "Epoch 1, Loss: 3.526376247406006, Accuracy: 7.418478488922119%, Test Loss: 3.7205257415771484, Test Accuracy: 3.515944242477417%\n",
      "Epoch 2, Loss: 3.4020886421203613, Accuracy: 8.614130020141602%, Test Loss: 3.6805684566497803, Test Accuracy: 5.342055320739746%\n",
      "Epoch 3, Loss: 3.293243408203125, Accuracy: 11.875%, Test Loss: 3.7665438652038574, Test Accuracy: 6.759335041046143%\n",
      "Epoch 4, Loss: 3.1581883430480957, Accuracy: 14.565217971801758%, Test Loss: 3.523942232131958, Test Accuracy: 8.394658088684082%\n",
      "Epoch 5, Loss: 2.9766478538513184, Accuracy: 18.478260040283203%, Test Loss: 3.4765403270721436, Test Accuracy: 9.484872817993164%\n",
      "Epoch 6, Loss: 2.78430438041687, Accuracy: 22.6358699798584%, Test Loss: 3.487335681915283, Test Accuracy: 11.610793113708496%\n",
      "Epoch 7, Loss: 2.550723075866699, Accuracy: 28.75%, Test Loss: 4.254498481750488, Test Accuracy: 10.684110641479492%\n",
      "Epoch 8, Loss: 2.352982521057129, Accuracy: 33.01630401611328%, Test Loss: 4.049651622772217, Test Accuracy: 10.329790115356445%\n",
      "Epoch 9, Loss: 2.1708028316497803, Accuracy: 37.33695602416992%, Test Loss: 4.021487236022949, Test Accuracy: 9.348596572875977%\n",
      "Epoch 10, Loss: 1.9918105602264404, Accuracy: 42.1467399597168%, Test Loss: 4.1260762214660645, Test Accuracy: 9.839193344116211%\n",
      "Epoch 11, Loss: 1.7124828100204468, Accuracy: 48.3967399597168%, Test Loss: 4.599857807159424, Test Accuracy: 10.248023986816406%\n",
      "Epoch 12, Loss: 1.401556372642517, Accuracy: 57.52717208862305%, Test Loss: 5.431062698364258, Test Accuracy: 10.302535057067871%\n",
      "Epoch 13, Loss: 1.194272518157959, Accuracy: 62.17391586303711%, Test Loss: 6.713125228881836, Test Accuracy: 10.057236671447754%\n",
      "Epoch 14, Loss: 1.0115313529968262, Accuracy: 68.61412811279297%, Test Loss: 6.275244235992432, Test Accuracy: 10.166257858276367%\n"
     ]
    }
   ],
   "source": [
    "epochs = 15\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Reset the metrics at the start of the next epoch\n",
    "    train_loss2.reset_states()\n",
    "    train_accuracy2.reset_states()\n",
    "    test_loss2.reset_states()\n",
    "    test_accuracy2.reset_states()\n",
    "\n",
    "    for train_images, train_labels in train_gen2:\n",
    "        train_step2(train_images, train_labels)\n",
    "\n",
    "    for test_images, test_labels in test_gen2:\n",
    "        test_step2(test_images, test_labels)\n",
    "    if epoch % 1 == 0:\n",
    "        print(\n",
    "          f'Epoch {epoch}, '\n",
    "          f'Loss: {train_loss2.result()}, '\n",
    "          f'Accuracy: {train_accuracy2.result() * 100}%, '\n",
    "          f'Test Loss: {test_loss2.result()}, '\n",
    "          f'Test Accuracy: {test_accuracy2.result() * 100}%'\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2e563b-4200-431a-87ae-66c744144e6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
